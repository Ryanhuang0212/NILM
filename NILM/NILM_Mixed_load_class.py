from __future__ import print_function
from matplotlib import pyplot as plt
import numpy as np
import pandas as pd
import os
from sklearn import preprocessing
import keras
import time
import shutil

# é–‹å§‹è¨ˆæ™‚
start_time = time.time()
print('åˆ†é¡ç¨‹å¼é–‹å§‹åŸ·è¡Œ=============================================')
print('keras version: ', keras.__version__)

# è¨­ç½®åƒæ•¸ =============================================================================================

Number_of_features = 10
window_size = 2000  # è¦–çª—å¤§å°
step_size = 2000  # è¦–çª—æ»‘å‹•æ­¥é•·
max_imfs = 10
level = 5

# ======================================================================================================

# è³‡æ–™é›†
csv_folder_path = r"D:\graduate_info\Research\code\lab load\2025 lab mix"
meta_json_path = r"D:\graduate_info\Research\code\lab load\2025 lab mix\2025_lab_15s_aggregated_Steam_juice.json"
input_folder = r"D:\graduate_info\Research\code\lab load\Devices_Mixed_load_Feature"

new_appliance_input_folder = r"D:\graduate_info\Research\code\lab load\new_appliance_output_folder_combine"

# å®šç¾©æ¸…ç©ºè³‡æ–™å¤¾çš„å‡½æ•¸
def clear_folder(folder_path):
    if os.path.exists(folder_path):
        for file in os.listdir(folder_path):
            file_path = os.path.join(folder_path, file)
            if os.path.isfile(file_path) or os.path.islink(file_path):
                os.unlink(file_path)  # åˆªé™¤æ–‡ä»¶æˆ–ç¬¦è™Ÿé€£çµ
            elif os.path.isdir(file_path):
                shutil.rmtree(file_path)  # éæ­¸åˆªé™¤è³‡æ–™å¤¾
        print(f"è³‡æ–™å¤¾ {folder_path} å·²æ¸…ç©º")
    else:
        os.makedirs(folder_path)  # å¦‚æœè³‡æ–™å¤¾ä¸å­˜åœ¨ï¼Œå‰µå»ºå®ƒ
        print(f"è³‡æ–™å¤¾ {folder_path} å·²å‰µå»º")

# è®€å–æ‰€æœ‰è¨­å‚™çš„ CSV æ–‡ä»¶
Device = []
valid_indices = []

for file_name in os.listdir(input_folder):
    if file_name.endswith(".csv"):
        file_path = os.path.join(input_folder, file_name)
        device_data = pd.read_csv(file_path)
        Device.append(device_data)

print(f"å·²æˆåŠŸè®€å– {len(Device)} å€‹è¨­å‚™çš„ CSV æ–‡ä»¶ã€‚")

all_data = pd.concat(Device, ignore_index=True)

new_appliance_data = {}
# å®šç¾©æ‚¨æ„Ÿèˆˆè¶£çš„ç‰¹å®šè² è¼‰åç¨±
target_appliances = ['Steam_Juice']

for file_name in os.listdir(new_appliance_input_folder):
    appliance_name = file_name.replace(".csv", "")
    file_path = os.path.join(new_appliance_input_folder, file_name)
    device_data = pd.read_csv(file_path)
    Device.append(device_data)

    new_appliance_data[appliance_name] = device_data

print(f"å·²æˆåŠŸè®€å– {len(new_appliance_data)} è¨­å‚™åç¨±ä¸¦å­˜å…¥new_appliance_dataã€‚")

# å»ºç«‹æ¯å€‹é›»å™¨çš„ WindowIndex é›†åˆ
valid_indices_per_appliance = {}

for i, appliance in enumerate(target_appliances):
    # ç²å–è©²è¨­å‚™çš„ `WindowIndex`
    valid_indices_per_appliance[appliance] = set(Device[i]["WindowIndex"].unique())

    print(f"ğŸ“Œ {appliance} çš„æœ‰æ•ˆ WindowIndex æ•¸é‡: {len(valid_indices_per_appliance[appliance])}")


print("âœ… æ‰€æœ‰é›»å™¨çš„æœ‰æ•ˆ WindowIndex å·²å­˜å…¥ `valid_indices_per_appliance`")

# ç”Ÿæˆè°æ³¢åˆ—å
harmonics_columns = [f'Harmonics{i+1}' for i in range(7)]
phase_columns = [f'phase{i + 1}' for i in range(7)] 

# å‡è¨­ä½ çš„æ•¸æ“šå­˜å„²åœ¨ all_data ä¸­
all_columns = ['RMS', 'Peak', 'Peak-to-Peak', 'Waveform Factor', 'Crest Factor', 
               'Power', 'Power Std'] + harmonics_columns + phase_columns + ['THD', 'Current Range','ZCR','vi area','GAF diag','P','Q','Skewness','Kurtosis','Delta Current Mean','Delta Current Std']

appliance_names = list(new_appliance_data.keys())
print(appliance_names)

# print('=================================================')
# æŒ‰ç…§ appliance_list é †åºï¼Œä¾åºå¾å­—å…¸å–å‡º DataFrame
devices = [new_appliance_data[name] for name in appliance_names]

# print('devices:',devices)
# print('=================================================')

# æå–æ‰€æœ‰ç‰¹å¾µ
feature_names = ['RMS', 'Peak', 'Peak-to-Peak', 'Waveform Factor', 'Crest Factor', 
                 'Power', 'Power Std', 'vi area', 'Current Range', 'ZCR', 
                'Skewness' , 'Kurtosis','Delta Current Mean','Delta Current Std'] 

# å®šç¾©è®ŠåŒ–é‡ç‰¹å¾µåç¨± (åŠ ä¸Š Delta_ å‰ç¶´)
delta_feature_names = [f"Delta_{name}" for name in feature_names]

# selected_features = ['P',  'Power', 'Q', 'THD']

features = feature_names

# æå–ç‰¹å¾µå’Œæ¨™ç±¤
X = all_data[features]

# ç›£ç£äºŒå…ƒåˆ†é¡
import joblib
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import load_model

num = 1
# ä½¿ç”¨  GMM é€²è¡Œèšé¡
gmm_model_path =  r"D:\graduate_info\Research\code\lab load\Machine Learning\GMMå½æ¨™ç±¤åˆ†é¡.pkl"

loaded_gmm = joblib.load(gmm_model_path)
# print("GMMé–‹å§‹é æ¸¬")
pseudo_labels = loaded_gmm.fit_predict(X)
# pseudo_labels = gmm_fake.fit_predict(X)
print("GMMé æ¸¬çµæŸ")

# å°†å‡æ¨™ç±¤æ·»åŠ åˆ°æ•°æ®ä¸­
all_data['pseudo_label'] = pseudo_labels
print("å½æ¨™ç±¤åˆ†å¸ƒ: ")
print(all_data['pseudo_label'].value_counts())

from sklearn.decomposition import PCA

# è¨­å®šè¦ä¿ç•™çš„ä¸»æˆåˆ†æ•¸é‡
pca = PCA(n_components = Number_of_features)  # ä¾‹å¦‚ä¿ç•™ 10 ç¶­ç‰¹å¾µ
X_pca = pca.fit_transform(X)

explained_variance = pca.explained_variance_ratio_
print(f"PCA ä¿ç•™äº† {X_pca.shape[1]} å€‹ä¸»æˆåˆ†")
print("PCA å„ä¸»æˆåˆ†çš„è²¢ç»åº¦:", explained_variance)

# æ‰¾å‡ºæœ€é‡è¦çš„ç‰¹å¾µ
num_features_to_select = min(len(features), X_pca.shape[1])  # ç¢ºä¿ä¸è¶…éåŸå§‹ç‰¹å¾µæ•¸
feature_importance = np.abs(pca.components_[:num_features_to_select, :]).sum(axis=0)  # è¨ˆç®—ç‰¹å¾µå½±éŸ¿åŠ›
pca_top_features = np.argsort(feature_importance)[-num_features_to_select:]  # å–å½±éŸ¿åŠ›æœ€å¤§çš„ç‰¹å¾µ

# é¸å–æœ€é‡è¦çš„ç‰¹å¾µ
selected_pca_features = [features[i] for i in pca_top_features]
print("âœ… PCA é¸å‡ºçš„ç‰¹å¾µ:", selected_pca_features)

top_features = selected_pca_features + harmonics_columns

print("top_features:", top_features)

# ä½¿ç”¨è®€å–çš„ç‰¹å¾µåç¨±é€²è¡Œå¾ŒçºŒæ“ä½œ
# ç¯©é¸æ•¸æ“šä¸­å°æ‡‰çš„ç‰¹å¾µåˆ—
selected_data = all_data[top_features]

print("ç¯©é¸å¾Œçš„æ•¸æ“š:")
print(selected_data.head())

# Step 4: ä½¿ç”¨ç­›é€‰ç‰¹å¾é‡æ–°èšç±»
X_selected = all_data[top_features].values
y_pseudo = np.array(pseudo_labels)

# åˆ†å‰²æ•¸æ“šé›†
X_train, X_test, y_train, y_test = train_test_split(X_selected, y_pseudo, test_size=0.2, random_state=42)

print("X_train shape:", X_train.shape)
print("X_test shape:", X_test.shape)
print("y_train shape:", y_train.shape)
print("y_test shape:", y_test.shape)

model_path =r'D:\graduate_info\Research\code\lab load\Machine Learning\æ—‹è½‰æ©Ÿå™¨5åˆ†é¡.h5'

model = load_model(model_path)

# æ¥ä¸‹ä¾†å°‡ y_pred_classes ç”¨æ–¼è¨­å‚™åˆ†é¡
class_1 = []
class_2 = []
class_3 = []
class_4 = []
class_5 = []
class_6 = []
class_7 = []
class_8 = []
class_9 = []
class_10 = []

class_1_features = []
class_2_features = []
class_3_features = []
class_4_features = []
class_5_features = []
class_6_features = []
class_7_features = []
class_8_features = []
class_9_features = []
class_10_features = []

class_1_delt_features = []

final_label_pred = []

def save_each_row_as_npy(dataframe, delt_dataframe, output_dir, prefix):
    os.makedirs(output_dir, exist_ok=True)
    # ç²å–ç‰¹å¾µå’Œè² è¼‰æ•¸æ“š
    features = dataframe.drop(columns=['LoadEncoded','load']).to_numpy()
    delt_features = delt_dataframe.drop(columns=['load']).to_numpy()
    load = dataframe['LoadEncoded'].to_numpy()

    for i, (feature_row, delt_feature_row ,load_value) in enumerate(zip(features, delt_features,load)):
        feature_row_reshaped = np.expand_dims(feature_row, axis=0)
        delt_feature_row_reshaped = np.expand_dims(delt_feature_row, axis=0)

        # ä¿å­˜å–®è¡Œç‰¹å¾µ
        feature_filename = os.path.join(output_dir, f"{prefix}_load_features_{i}.npy")
        np.save(feature_filename, feature_row_reshaped)
        delt_feature_filename = os.path.join(output_dir, f"{prefix}_load_delt_features_{i}.npy")
        np.save(delt_feature_filename, delt_feature_row_reshaped)

        # ä¿å­˜å°æ‡‰è² è¼‰
        load_filename = os.path.join(output_dir, f"{prefix}_load_name_{i}.npy")
        np.save(load_filename, np.array([load_value]))
    
    print(f"{prefix} çš„æ¯ä¸€è¡Œç‰¹å¾µã€è®ŠåŒ–é‡å’Œè² è¼‰å·²åˆ†åˆ«ä¿å­˜è‡³ {output_dir}")

def save_load_column_as_csv(dataframe, output_csv_path):
    if 'load' not in dataframe.columns:
        print(f"DataFrame è£¡æ²’æœ‰ 'load' æ¬„ä½ï¼Œç„¡æ³•è¼¸å‡º")
        return
    
    # åªå– 'load' æ¬„ä½ + å»é‡ (ç¢ºä¿æ¯å€‹åç¨±åªå‡ºç¾ä¸€æ¬¡)
    load_series = dataframe['load'].drop_duplicates()
    # å°‡å…¶è½‰æˆä¸€å€‹ DataFrame
    load_df = pd.DataFrame(load_series, columns=['load'])
    
    load_df.to_csv(output_csv_path, index=False, header=False, encoding='utf-8-sig')
    print(f"å·²å°‡å”¯ä¸€çš„é›»å™¨åç¨±å­˜æˆ CSVï¼š{output_csv_path}")

print('=====================================================================')

from PyEMD import EMD
from scipy.signal import hilbert
import pywt


class_1_output_dir = r'D:\graduate_info\Research\code\lab load\Class\Class 1_mix'  
class_2_output_dir = r'D:\graduate_info\Research\code\lab load\Class\Class 2_mix' 
class_3_output_dir = r'D:\graduate_info\Research\code\lab load\Class\Class 3_mix' 
class_4_output_dir = r'D:\graduate_info\Research\code\lab load\Class\Class 4_mix'
class_5_output_dir = r'D:\graduate_info\Research\code\lab load\Class\Class 5_mix' 
class_6_output_dir = r'D:\graduate_info\Research\code\lab load\Class\Class 6_mix'  
class_7_output_dir = r'D:\graduate_info\Research\code\lab load\Class\Class 7_mix' 
class_8_output_dir = r'D:\graduate_info\Research\code\lab load\Class\Class 8_mix' 
class_9_output_dir = r'D:\graduate_info\Research\code\lab load\Class\Class 9_mix' 
class_10_output_dir = r'D:\graduate_info\Research\code\lab load\Class\Class 10_mix' 


def wavelet_transform(data, wavelet='coif5', level = 5):
    coeffs = pywt.wavedec(data, wavelet, level=level)
    flattened_coeffs = np.hstack(coeffs)
    return flattened_coeffs

def HHT_transform(data, max_imfs = 10):
    emd = EMD()
    imfs = emd(data)
    # è£œ 0 åˆ°æŒ‡å®šæ•¸é‡ / æˆªæ–·
    if len(imfs) < max_imfs:
        padding = max_imfs - len(imfs)
        imfs = np.pad(imfs, ((0, padding), (0, 0)), 'constant')
    imfs = imfs[:max_imfs]
    
    hht_data = []
    for imf in imfs:
        analytic_signal = hilbert(imf)
        amplitude_envelope = np.abs(analytic_signal)
        instantaneous_phase = np.unwrap(np.angle(analytic_signal))
        instantaneous_frequency = np.diff(instantaneous_phase) / (2.0 * np.pi) * 1000
        # amplitude & frequency
        hht_data.append(amplitude_envelope)
        hht_data.append(instantaneous_frequency)
    
    # flatten
    flattened_hht = np.hstack(hht_data)
    return flattened_hht

def sliding_window_feature_extraction(data_array, label_array,
                                      window_size=5000, step=2500,
                                      wavelet='coif5', level=5,
                                      max_imfs=20,
                                      valid_indices=None):
    n_samples = len(data_array)
    HHT_features_list = []
    DWT_features_list = []
    window_labels_list = []
    window_index = 0 
    
    for start in range(0, n_samples - window_size + 1, step):
        end = start + window_size
         # è‹¥æŒ‡å®šäº† valid_indicesï¼Œä¸”ç›®å‰çš„ window_index ä¸åœ¨å…¶ä¸­ => ç›´æ¥è·³é
        if (valid_indices is not None) and (window_index not in valid_indices):
            window_index += 1
            continue
        # 1) å–è©²çª—è¨Šè™Ÿ
        window_signal = data_array[start:end]
        # 2) å–è©²çª—å°æ‡‰çš„æ¨™ç±¤åºåˆ—
        window_label_seq = label_array[start:end]
        
        # ---- åŸ·è¡Œ HHT ç‰¹å¾µ ----
        hht_fv = HHT_transform(window_signal, max_imfs=max_imfs)
        # ---- åŸ·è¡Œ DWT ç‰¹å¾µ ----
        dwt_fv = wavelet_transform(window_signal, wavelet=wavelet, level=level)

        # ---- æ±ºå®šè©²çª—çš„æ•´é«”æ¨™ç±¤ (e.g. å¤šæ•¸æ±º) ----
        #   é€™è£¡ç°¡å–®ç”¨ value_counts().idxmax()
        unique_counts = {}
        for lbl in window_label_seq:
            unique_counts[lbl] = unique_counts.get(lbl, 0) + 1
        majority_label = max(unique_counts, key=unique_counts.get)

        # ---- æ”¶é›†ç‰¹å¾µ + æ¨™ç±¤ ----
        HHT_features_list.append(hht_fv)
        DWT_features_list.append(dwt_fv)
        window_labels_list.append(majority_label)
        window_index += 1
    
    # è½‰æˆ numpy array
    HHT_features = np.array(HHT_features_list)
    DWT_features = np.array(DWT_features_list)
    labels = np.array(window_labels_list)
    
    return HHT_features, DWT_features, labels

# æª¢æŸ¥å½¢ç‹€æ˜¯å¦ä¸€è‡´çš„å‡½æ•¸
def check_and_save(data, filepath, expected_shape=None):
    if expected_shape is None:
        expected_shape = data.shape
    elif data.shape != expected_shape:
        print(f"å½¢ç‹€ä¸ä¸€è‡´ï¼é æœŸå½¢ç‹€: {expected_shape}, å¯¦éš›å½¢ç‹€: {data.shape}")
        raise ValueError("æ•¸æ“šå½¢ç‹€ä¸ä¸€è‡´ï¼Œåœæ­¢ä¿å­˜ã€‚")
    np.save(filepath, data)
    
print('=====================================================================')

for i, (device, group) in enumerate(zip(appliance_names, Device)):
    # æå–èˆ‡æ¨¡å‹ä¸€è‡´çš„ç‰¹å¾µ
    X_group = group[top_features].values
    
    # é æ¸¬æ¯å€‹æ¨£æœ¬çš„é¡åˆ¥
    group_pred = model.predict(X_group)  # é æ¸¬æ¦‚ç‡
    mean_probs = np.mean(group_pred, axis=0) 

    cluster_label = np.argmax(mean_probs)  
    
    # ä¿å­˜çµæœ
    final_label_pred.append(cluster_label)
    print(f"è¨­å‚™ {device} çš„èšé¡çµæœ: {cluster_label}")

# æª¢æŸ¥æœ€çµ‚çš„åˆ†é¡çµæœ
print('final_label_pred:', final_label_pred)

final_label_pred = np.array(final_label_pred)
# print("æœ€çµ‚åˆ†é¡çµæœ:", final_label_pred)

LABEL = 'LoadEncoded'
print(LABEL)
le = preprocessing.LabelEncoder()

for i, label in enumerate(final_label_pred):
    device_data = Device[i][top_features].copy()
    device_delt_data = Device[i][delta_feature_names].copy()
    if label == 0:
        class_1.append(devices[i])
        device_data['load'] = appliance_names[i]
        class_1_features.append(device_data)
        device_delt_data['load'] = appliance_names[i]
        class_1_delt_features.append(device_delt_data)
        print("class_1_delt_features:",class_1_delt_features)

    elif label == 1:
        class_2.append(devices[i])
        device_data['load'] = appliance_names[i]
        class_2_features.append(device_data)

    elif label == 2:
        class_3.append(devices[i])
        device_data['load'] = appliance_names[i]
        class_3_features.append(device_data)

    elif label == 3:
        class_4.append(devices[i])
        device_data['load'] = appliance_names[i]
        class_4_features.append(device_data)

    elif label == 4:
        class_5.append(devices[i])
        device_data['load'] = appliance_names[i]
        class_5_features.append(device_data)

    elif label == 5:
        class_6.append(devices[i])
        device_data['load'] = appliance_names[i]
        class_6_features.append(device_data)

    elif label == 6:
        class_7.append(devices[i])
        device_data['load'] = appliance_names[i]
        class_7_features.append(device_data)

    elif label == 7:
        class_8.append(devices[i])
        device_data['load'] = appliance_names[i]
        class_8_features.append(device_data)

    elif label == 8:
        class_9.append(devices[i])
        device_data['load'] = appliance_names[i]
        class_9_features.append(device_data)
        
    elif label == 9:
        class_10.append(devices[i])
        device_data['load'] = appliance_names[i]
        class_10_features.append(device_data)

print("é–‹å§‹åš DWT èˆ‡ HHT ")

def process_single_device(
    device_df, 
    device_features_df, 
    delt_device_features_df,
    output_dir,
    device_name,
    start_index=0, 
    LABEL='LoadEncoded',      # é€™æ˜¯æˆ‘å€‘æ¯æ¬¡å‘¼å«æ™‚çš„ã€Œèµ·å§‹æ‰¹æ¬¡ç·¨è™Ÿã€
    wavelet='coif5',
    level=5,
    max_imfs=20,
    window_size=2000,
    step=2000,
    num = num,
    valid_indices=None 
):
   
    now_device = device_df["load"].iloc[0]
    print("now_device",now_device)

    le = preprocessing.LabelEncoder()
    device_df[LABEL] = le.fit_transform(device_df['load'].values.ravel())
    device_features_df[LABEL] = le.fit_transform(device_features_df['load'].values.ravel())

    # ç‚ºäº† LabelEncode 'load'

    # (å¯é¸) è¼¸å‡ºæ¯è¡Œç‰¹å¾µ
    save_each_row_as_npy(device_features_df, delt_device_features_df, output_dir, prefix=f"class{num}_{device_name}")
    
    # åšç°¡å–®åœ–è¡¨ (å¯é¸)
    plt.figure(figsize=(6, 4))
    device_df['load'].value_counts().plot(kind='bar', title=device_name)
    plt.tight_layout()
    save_path = os.path.join(output_dir, f'{device_name}.png')
    plt.savefig(save_path, dpi=200)
    plt.close()

    # sliding window
    data_array = device_df['current'].values.astype(float)
    label_array = device_df[LABEL].values.astype(int)
    valid_indices = valid_indices_per_appliance[now_device]

    HHT_features, DWT_features, labels = sliding_window_feature_extraction(
        data_array=data_array,
        label_array=label_array,
        window_size=window_size,
        step=step,
        wavelet=wavelet,
        level=level,
        max_imfs=max_imfs,
        valid_indices=valid_indices 

    )

    # å°‡çµæœé€ batch è¼¸å‡ºï¼Œä½†ç·¨è™Ÿå¾ start_index é–‹å§‹
    batch_counter = start_index
    for i in range(len(labels)):
        hht_i = np.expand_dims(HHT_features[i], axis=0)
        dwt_i = np.expand_dims(DWT_features[i], axis=0)
        label_i = np.array([labels[i]])

        np.save(os.path.join(output_dir, f"features_HHT_batch_{batch_counter}.npy"), hht_i)
        np.save(os.path.join(output_dir, f"features_DWT_batch_{batch_counter}.npy"), dwt_i)
        np.save(os.path.join(output_dir, f"labels_batch_{batch_counter}.npy"), label_i)

        batch_counter += 1  # ä¾åºéå¢

    # å›å‚³ã€Œæœ¬è¨­å‚™è™•ç†çµæŸå¾Œçš„ batch_counterã€
    return batch_counter

def class_process(num, class_num, class_features, class_delt_features,class_output_dir, 
                  wavelet='coif5', level=5,
                  max_imfs=20, window_size=5000, step=2500,
                  valid_indices=None):
    if not class_num:  # å¦‚æœæ²’æœ‰æ­¤é¡åˆ¥
        clear_folder(class_output_dir)
        print(f"æ²’æœ‰æ‰¾åˆ°ä»»ä½• class{num}ã€‚")
        return
    
    # å»ºç«‹ / æ¸…ç©º class X ç¸½ç›®éŒ„ï¼ˆå¯é¸ï¼Œçœ‹æ‚¨æ˜¯å¦è¦é€™æ¨£åšï¼‰
    os.makedirs(class_output_dir, exist_ok=True)
    clear_folder(class_output_dir)

    batch_counter = 0  # å¾ 0 é–‹å§‹
    
    for i, (device_df, device_features_df, delt_device_features_df) in enumerate(zip(class_num, class_features, class_delt_features)):
        device_name = device_df['load'].iloc[0]  # å–ç¬¬ä¸€ç­† load ä½œè¨­å‚™åç¨±

        # å‘¼å«è™•ç†å‡½å¼ï¼Œä¸¦æŠŠ batch_counter ç•¶æˆ start_index å‚³é€²å»
        batch_counter = process_single_device(
            device_df=device_df.copy(),
            device_features_df=device_features_df.copy(),
            delt_device_features_df = delt_device_features_df.copy(),
            output_dir=class_output_dir,
            device_name=f"{device_name}_device{i}",
            start_index = batch_counter,  # é€™è£¡æŠŠç›®å‰çš„è¨ˆæ•¸å¸¶é€²å»
            wavelet = wavelet,
            level = level,
            max_imfs = max_imfs,
            window_size = window_size,
            step = step,
            num = num,
            valid_indices=valid_indices
        )
        # process_single_device æœƒå›å‚³ã€Œæœ€æ–°çš„ batch_counterã€
        # é€™æ¨£ä¸‹ä¸€å°è¨­å‚™å°±æœƒå¾ä¸Šä¸€æ¬¡çµæŸçš„ç·¨è™Ÿç¹¼çºŒåŠ 
        
    print(f"Class{num}ï¼šæ‰€æœ‰è¨­å‚™éƒ½å·²è™•ç†å®Œç•¢ï¼Œæœ€çµ‚ batch_counter={batch_counter}")

class_lists = {
    1: class_1,
    2: class_2,
    3: class_3,
    4: class_4,
    5: class_5,
    6: class_6,
    7: class_7,
    8: class_8,
    9: class_9,
    10: class_10
}

class_features_lists = {
    1: class_1_features,
    2: class_2_features,
    3: class_3_features,
    4: class_4_features,
    5: class_5_features,
    6: class_6_features,
    7: class_7_features,
    8: class_8_features,
    9: class_9_features,
    10: class_10_features
}

class_delt_features_lists = {
    1: class_1_delt_features
}

output_dirs_lists = {
    1: class_1_output_dir,
    2: class_2_output_dir,
    3: class_3_output_dir,
    4: class_4_output_dir,
    5: class_5_output_dir,
    6: class_6_output_dir,
    7: class_7_output_dir,
    8: class_8_output_dir,
    9: class_9_output_dir,
    10: class_10_output_dir
}

for classnumber in range(1, num + 1):
    class_process( 
        num = classnumber,
        class_num = class_lists[classnumber],
        class_features = class_features_lists[classnumber],
        class_delt_features = class_delt_features_lists[classnumber],
        class_output_dir = output_dirs_lists[classnumber],
        wavelet='coif5',
        level=level,
        max_imfs=max_imfs,
        window_size=window_size,
        step=step_size,
        valid_indices=valid_indices
        )

# çµæŸè¨ˆæ™‚
end_time = time.time()

# è¨ˆç®—ç¸½åŸ·è¡Œæ™‚é–“                
execution_time = end_time - start_time
print(f"ç¨‹å¼åŸ·è¡Œæ™‚é–“: {execution_time} ç§’")